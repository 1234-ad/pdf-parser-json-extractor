#!/usr/bin/env python3
"""
PDF Parser and JSON Extractor
A Python program that parses PDF files and extracts content into structured JSON format.

Author: Generated by Bhindi AI
Date: September 2025
"""

import json
import re
import sys
import argparse
from typing import Dict, List, Any, Optional
import requests
from io import BytesIO

class PDFParser:
    """
    A comprehensive PDF parser that extracts content and structures it into JSON format.
    Supports text, tables, and basic chart detection.
    """
    
    def __init__(self):
        self.content_types = {
            'paragraph': self._is_paragraph,
            'table': self._is_table,
            'chart': self._is_chart
        }
    
    def parse_pdf(self, pdf_path_or_url: str) -> Dict[str, Any]:
        """
        Main method to parse PDF and return structured JSON
        
        Args:
            pdf_path_or_url: Path to PDF file or URL
            
        Returns:
            Dictionary containing structured PDF content
        """
        try:
            # Import required libraries
            import pdfplumber
            try:
                import camelot
                camelot_available = True
            except ImportError:
                camelot_available = False
                print("Warning: camelot-py not available. Table extraction will be limited.")
            
            # Handle URL or file path
            if pdf_path_or_url.startswith('http'):
                print(f"Downloading PDF from URL: {pdf_path_or_url}")
                response = requests.get(pdf_path_or_url)
                response.raise_for_status()
                pdf_file = BytesIO(response.content)
            else:
                pdf_file = pdf_path_or_url
            
            result = {
                "pages": []
            }
            
            print("Extracting content from PDF...")
            
            # Extract text and basic structure with pdfplumber
            with pdfplumber.open(pdf_file) as pdf:
                total_pages = len(pdf.pages)
                print(f"Processing {total_pages} pages...")
                
                for page_num, page in enumerate(pdf.pages, 1):
                    print(f"Processing page {page_num}/{total_pages}")
                    page_content = self._extract_page_content(page, page_num)
                    result["pages"].append(page_content)
            
            # Extract tables with camelot (for file paths only)
            if camelot_available and not pdf_path_or_url.startswith('http'):
                try:
                    print("Extracting tables with Camelot...")
                    tables = camelot.read_pdf(pdf_path_or_url, pages='all')
                    self._integrate_tables(result, tables)
                    print(f"Found {len(tables)} tables")
                except Exception as e:
                    print(f"Table extraction with Camelot failed: {e}")
            
            print("PDF parsing completed successfully!")
            return result
            
        except Exception as e:
            print(f"Error parsing PDF: {e}")
            return {"error": str(e), "pages": []}
    
    def _extract_page_content(self, page, page_num: int) -> Dict[str, Any]:
        """Extract content from a single page"""
        page_data = {
            "page_number": page_num,
            "content": []
        }
        
        # Extract text
        text = page.extract_text()
        if text:
            # Split into blocks and analyze
            blocks = self._split_into_blocks(text)
            for block in blocks:
                content_item = self._analyze_block(block)
                if content_item:
                    page_data["content"].append(content_item)
        
        # Extract tables from page using pdfplumber
        tables = page.extract_tables()
        for i, table in enumerate(tables):
            if table and len(table) > 1:  # Valid table with header and data
                # Clean table data
                cleaned_table = []
                for row in table:
                    cleaned_row = [cell.strip() if cell else "" for cell in row]
                    if any(cleaned_row):  # Skip empty rows
                        cleaned_table.append(cleaned_row)
                
                if cleaned_table:
                    table_item = {
                        "type": "table",
                        "section": self._detect_section_from_table(cleaned_table),
                        "description": None,
                        "table_data": cleaned_table
                    }
                    page_data["content"].append(table_item)
        
        return page_data
    
    def _split_into_blocks(self, text: str) -> List[str]:
        """Split text into logical blocks"""
        # Split by double newlines or major section breaks
        blocks = re.split(r'\n\s*\n', text.strip())
        return [block.strip() for block in blocks if block.strip()]
    
    def _analyze_block(self, block: str) -> Optional[Dict[str, Any]]:
        """Analyze a text block and determine its type"""
        if not block or len(block.strip()) < 10:
            return None
        
        # Detect section headers
        section, sub_section = self._detect_sections(block)
        
        # Determine content type
        content_type = self._determine_content_type(block)
        
        if content_type == 'paragraph':
            return {
                "type": "paragraph",
                "section": section,
                "sub_section": sub_section,
                "text": block.strip()
            }
        elif content_type == 'chart':
            return {
                "type": "chart",
                "section": section,
                "description": block.strip(),
                "chart_data": self._extract_chart_data(block)
            }
        
        return {
            "type": "paragraph",
            "section": section,
            "sub_section": sub_section,
            "text": block.strip()
        }
    
    def _detect_sections(self, text: str) -> tuple:
        """Detect section and sub-section from text"""
        lines = text.split('\n')
        section = None
        sub_section = None
        
        # Look for section headers (usually short lines, possibly with formatting)
        for line in lines[:3]:  # Check first few lines
            line = line.strip()
            if len(line) < 100 and len(line) > 5:
                # Check if it looks like a header
                if (line.isupper() or 
                    line.title() == line or 
                    re.match(r'^[A-Z][a-z\s]+$', line) or
                    re.match(r'^\d+\.?\s+[A-Z]', line)):
                    if not section:
                        section = line
                    elif not sub_section:
                        sub_section = line
        
        return section, sub_section
    
    def _determine_content_type(self, text: str) -> str:
        """Determine if text is paragraph, table, or chart"""
        # Chart indicators
        chart_keywords = ['chart', 'graph', 'plot', 'figure', 'diagram', 'visualization']
        if any(keyword in text.lower() for keyword in chart_keywords):
            return 'chart'
        
        # Table indicators (handled separately in extract_tables)
        table_keywords = ['table', 'data', 'column', 'row']
        if any(keyword in text.lower() for keyword in table_keywords) and '\t' in text:
            return 'table'
        
        return 'paragraph'
    
    def _extract_chart_data(self, text: str) -> List[List[str]]:
        """Extract basic chart data if present"""
        # Look for data patterns in text
        lines = text.split('\n')
        data = []
        
        for line in lines:
            # Look for year/value patterns
            if re.search(r'\d{4}.*\$?\d+[MK]?', line):
                parts = re.findall(r'(\d{4}|\$?\d+[MK]?)', line)
                if len(parts) >= 2:
                    data.append(parts)
        
        return data if data else [["XLabel", "YLabel"]]
    
    def _detect_section_from_table(self, table: List[List[str]]) -> str:
        """Detect section name for a table"""
        if table and table[0]:
            # Use first row as potential section indicator
            first_row = ' '.join(table[0])
            if 'revenue' in first_row.lower() or 'financial' in first_row.lower():
                return "Financial Data"
            elif 'performance' in first_row.lower():
                return "Performance Overview"
            elif 'year' in first_row.lower():
                return "Yearly Data"
        return "Data Table"
    
    def _integrate_tables(self, result: Dict, tables) -> None:
        """Integrate camelot-extracted tables into result"""
        for table in tables:
            if table.df is not None and not table.df.empty:
                table_data = table.df.values.tolist()
                # Add headers
                headers = table.df.columns.tolist()
                table_data.insert(0, headers)
                
                # Find appropriate page to add table
                page_num = table.page - 1  # Convert to 0-based index
                if page_num < len(result["pages"]):
                    table_item = {
                        "type": "table",
                        "section": self._detect_section_from_table(table_data),
                        "description": f"Table extracted from page {table.page}",
                        "table_data": table_data
                    }
                    result["pages"][page_num]["content"].append(table_item)
    
    def _is_paragraph(self, text: str) -> bool:
        """Check if text is a paragraph"""
        return len(text.split()) > 5 and not self._is_table(text) and not self._is_chart(text)
    
    def _is_table(self, text: str) -> bool:
        """Check if text represents a table"""
        return '\t' in text or '|' in text or re.search(r'\s{3,}', text)
    
    def _is_chart(self, text: str) -> bool:
        """Check if text describes a chart"""
        chart_keywords = ['chart', 'graph', 'plot', 'figure', 'diagram']
        return any(keyword in text.lower() for keyword in chart_keywords)


def main():
    """Main function to handle command line arguments and run the parser"""
    parser = argparse.ArgumentParser(
        description='Parse PDF files and extract content to structured JSON format'
    )
    parser.add_argument(
        'input_pdf', 
        help='Path to PDF file or URL'
    )
    parser.add_argument(
        '-o', '--output', 
        default='output.json',
        help='Output JSON file path (default: output.json)'
    )
    parser.add_argument(
        '--pretty', 
        action='store_true',
        help='Pretty print JSON output'
    )
    
    args = parser.parse_args()
    
    # Initialize parser
    pdf_parser = PDFParser()
    
    # Parse PDF
    print(f"Parsing PDF: {args.input_pdf}")
    result = pdf_parser.parse_pdf(args.input_pdf)
    
    # Save to JSON
    try:
        with open(args.output, 'w', encoding='utf-8') as f:
            if args.pretty:
                json.dump(result, f, indent=2, ensure_ascii=False)
            else:
                json.dump(result, f, ensure_ascii=False)
        
        print(f"\nResults saved to: {args.output}")
        
        # Print summary
        if "pages" in result:
            total_pages = len(result["pages"])
            total_content = sum(len(page.get("content", [])) for page in result["pages"])
            print(f"Summary: {total_pages} pages processed, {total_content} content items extracted")
        
    except Exception as e:
        print(f"Error saving results: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())